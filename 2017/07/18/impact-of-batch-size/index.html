<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Deep Learning,Machine Learning," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="From Zhihu.">
<meta name="keywords" content="Deep Learning,Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="How the size of batch size impact on the learning effect.">
<meta property="og:url" content="IamTao.github.io/2017/07/18/impact-of-batch-size/index.html">
<meta property="og:site_name" content="庭草交翠">
<meta property="og:description" content="From Zhihu.">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_b.png">
<meta property="og:updated_time" content="2018-09-01T13:04:23.007Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="How the size of batch size impact on the learning effect.">
<meta name="twitter:description" content="From Zhihu.">
<meta name="twitter:image" content="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_b.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="IamTao.github.io/2017/07/18/impact-of-batch-size/"/>





  <title>How the size of batch size impact on the learning effect. | 庭草交翠</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">庭草交翠</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="IamTao.github.io/2017/07/18/impact-of-batch-size/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="虫二">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="庭草交翠">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">How the size of batch size impact on the learning effect.</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-18T16:14:17+02:00">
                2017-07-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Science/" itemprop="url" rel="index">
                    <span itemprop="name">Data Science</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>From <a href="https://www.zhihu.com/question/32673260" target="_blank" rel="noopener">Zhihu</a>.</p>
<a id="more"></a>
<p>Batch 的选择，首先决定的是下降的方向。<br>如果数据集比较小，可以采用<strong>全数据集</strong>(Full Batch Learning)，这样有两个好处：</p>
<ul>
<li>由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。</li>
<li>这样理论上是可以得到全局收敛的</li>
<li>可以用一些加速收敛算法，比如L-BFGS之类的</li>
<li>便于并行计算但是同样存在缺点：如果数据集过大，训练会很慢的</li>
</ul>
<p>但对于更大的数据，以上2个好处又变成两个坏处：</p>
<ul>
<li>随着数据集的海量增加和内存限制，一次性载入所有的数据进来变得越来越不可行。</li>
<li>收敛速度是比较快，但比较容易陷入局部最小情况。太大的batch size 容易陷入sharp minima，泛化性不好。</li>
</ul>
<p>既然Full Batch Learning 并不适用大数据，那么我们走向了另一个极端，即，每次只训练一个样本，即<code>Batch Size = 1</code>，即online learning。他有如下优点：</p>
<ul>
<li>每个采样上的参数修正方向会与整体最优的方向有出入。这条看似是个缺点，实际上，DNN因为是非线性模型，有很多参数，现有目标函数会有很多局部极值，这就会导致模型不精确。<ul>
<li><code>Full Batch Learning</code>方法每次修正依赖于现有模型和所有数据，很难跳出这些局部极值，所以Full Batch Learning方法是一种很依赖于初始模型的方法。</li>
<li><code>Online learning</code>方法基于每个采样去修正，修正幅度大了以后，就容易跳出这些局部极值，避免过拟合发生。该方法一般不依赖于初始模型，所以可以用来训练初始的神经网络。</li>
</ul>
</li>
<li>速度比<code>Full Batch Learning</code>快。</li>
</ul>
<p>但同时又有如下缺点：</p>
<ul>
<li>难以并行计算</li>
<li>因为每次更新基于单个采样，很容易导致难以收敛。这个发生原因和优点1 是一样的。这种现象有时是优点有时也是缺点，主要取决于学习率的选择。<ul>
<li>线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元，非线性网络，在具备依然近似是抛物面。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞，难以达到收敛。</li>
</ul>
</li>
</ul>
<p><img src="https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_b.png" alt=""></p>
<p>于是我们选择了一个适中的<code>Batch Size</code> (Mini-batches Learning)。如果数据集足够充分，那么用部分数据训练得出的梯度与用全部数据训练出来的梯度几乎是一样的。</p>
<p>在合理范围内，增大Batch Size有如下好处：</p>
<ul>
<li>内存利用率提高了，大矩阵乘法的并行化效率提高。</li>
<li>跑完一次epoch (全数据集)所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</li>
<li>在一定范围内，一般来说，Batch Size 越大，其确定的下降方向越准，引起训练震荡越小。</li>
<li>批训练的引入最大好处是针对非凸损失函数来做的，毕竟非凸的情况下，全样本就算工程上算的动，也会卡在局部优上，批表示了全样本的部分抽样实现，相当于人为引入修正梯度上的采样噪声，使“一路不通找别路”更有可能搜索最优值。</li>
<li>小batch size引入的随机性会更大些，有时候能有更好的效果，但是收敛速度比较慢。</li>
</ul>
<p>盲目增大Batch Size 有何坏处？</p>
<ul>
<li>内存利用率提高了，但是内存容量可能撑不住了。</li>
<li>跑完一次epoch所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</li>
<li>Batch Size增大到一定程度，其确定的下降方向已经基本不再变化。</li>
</ul>
<p>但值得注意的是：随着batch normalization的普及，收敛速度已经不像前bn时代一样需要比较玄学的调参，现在一般还是采用大batch size。但bn的坏处就是不能用太小的batch size，不然mean和variance就偏了。</p>
<h1 id="Tradeoff-batch-size-v-s-number-of-iterations"><a href="#Tradeoff-batch-size-v-s-number-of-iterations" class="headerlink" title="Tradeoff batch size v.s. number of iterations"></a>Tradeoff batch size v.s. number of iterations</h1><h2 id="Ref-1"><a href="#Ref-1" class="headerlink" title="Ref 1"></a>Ref 1</h2><p>From <a href="https://arxiv.org/abs/1609.04836" target="_blank" rel="noopener">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a>.</p>
<p>The stochastic gradient descent method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, usually 32–512 data points, is sampled to compute an approximation to the gradient.</p>
<blockquote>
<p>It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize.</p>
</blockquote>
<p>The paper presents ample numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions – and that sharp minima lead to poorer generalization.<br>In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.</p>
<ul>
<li>These minimizers are characterized by large positive eigenvalues in ∇^2 f(x) and tend to generalize less well.</li>
<li>In contrast, small-batch methods converge to flat minimizers characterized by small positive eigenvalues of ∇^2 f(x).</li>
<li>We have observed that the loss function landscape of deep neural networks is such that large-batch methods are almost invariably attracted to regions with sharp minima and that, unlike small batch methods, are unable to escape basins of these minimizers.</li>
</ul>
<h2 id="Ref-2"><a href="#Ref-2" class="headerlink" title="Ref 2"></a>Ref 2</h2><p>Some good insights from Ian Goodfellow answering to <a href="https://www.quora.com/Deep-learning-why-do-not-use-the-whole-training-set-to-compute-the-gradient" target="_blank" rel="noopener">why do not use the whole training set to compute the gradient?</a> on Quora:</p>
<ul>
<li>The size of the learning rate is limited mostly by factors like how curved the cost function is. You can think of gradient descent as making a linear approximation to the cost function, then moving downhill along that approximate cost. If the cost function is highly non-linear (highly curved) then the approximation will not be very good for very far, so only small step sizes are safe. Please check <a href="http://www.deeplearningbook.org/contents/optimization.html" target="_blank" rel="noopener">the Chapter 4 of the deep learning textbook</a> for more details.</li>
<li>When you put m examples in a minibatch, you need to do O(m) computation and use O(m) memory, but you reduce the amount of uncertainty in the gradient by a factor of only O(sqrt(m)). In other words, there are diminishing marginal returns to putting more examples in the minibatch. Please check <a href="http://www.deeplearningbook.org/contents/optimization.html" target="_blank" rel="noopener">the Chapter 8 of the deep learning textbook</a> for more details.</li>
<li>Also, if you think about it, even using the entire training set doesn’t really give you the true gradient. The true gradient would be the expected gradient with the expectation taken over all possible examples, weighted by the data generating distribution. Using the entire training set is just using a very large minibatch size, where the size of your minibatch is limited by the amount you spend on data collection, rather than the amount you spend on computation.</li>
</ul>
<h2 id="Ref-3"><a href="#Ref-3" class="headerlink" title="Ref 3"></a>Ref 3</h2><p>We are talk about ‘reducing the batch size in a mini batch stochastic gradient descent algorithm’ V.S. ‘larger batch sizes requiring fewer iterations’.</p>
<p>Let’s take the two extremes cases:</p>
<ul>
<li>Each gradient descent step is using the entire dataset.<ul>
<li>You’re computing the gradients for every sample. In this case you know exactly the best directly towards a local minimum. You don’t waste time going the wrong direction.</li>
<li>So in terms of numbers gradient descent steps, you’ll get there in the fewest.</li>
</ul>
</li>
<li>A batch size of just 1 sample.<ul>
<li>The gradient of that sample may take you completely the wrong direction.</li>
<li>But the cost of computing the one gradient was quite trivial. As you take steps with regard to just one sample you “wander” around a bit, but on the average you head towards the same local minimum as in full batch gradient descent.</li>
</ul>
</li>
</ul>
<p>In terms of computational power, while the single-sample stochastic GD process takes many many more iterations, you end up getting there for less cost than the full batch mode, “typically.”</p>
<p>We might realize that:</p>
<ul>
<li>modern BLAS libraries make computing vector math quite efficient, so computing 10 or 100 samples at once, presuming you’ve vectorized your code properly, will be barely more work than computing 1 sample (you gain memory call efficiencies as well as computational tricks built into most efficient math libraries).</li>
<li>averaging over a batch of 10, 100, 1000 samples is going to produce a gradient that is a more reasonable approximation of the true, full batch-mode gradient.<ul>
<li>our steps are now more accurate, meaning we need fewer of them to converge, and at a cost that is only marginally higher than single-sample GD.</li>
</ul>
</li>
</ul>
<h2 id="Ref-4-batch-size-related-to-hardware"><a href="#Ref-4-batch-size-related-to-hardware" class="headerlink" title="Ref 4: batch size related to hardware"></a>Ref 4: batch size related to hardware</h2><p>Opinion 1:</p>
<ul>
<li>In general GPUs offer the best performance when they are computing matrix operations over large contiguous matrices. Larger batch sizes yield larger input/data matrices and thus better performance.</li>
<li>There is nothing special about powers of two for batchsizes. You can use the maximum batchsize that fits on your GPU/RAM to train it so that you utilize it to the fullest. The entire point of batchwise training is that the training is fast and minibatch noise is added to SGD, both of these have no relation to “powers of two” batch size.<ul>
<li>Note that image resized to power of two makes sense (because pooling is generally done in 2X2 windows), but that’s a different thing altogether.</li>
</ul>
</li>
</ul>
<p>Opinion 2:</p>
<ul>
<li>The deep learning book indicates that some kinds of harware achieve better runtime with sepcific size of arrays. Especially when using GPUs, it is common for power of 2 batch sizes to offer better runtime.</li>
<li>Someone indicates that powers of 2 have some advantages with regards to vectorized operations in certain packages.</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/07/11/python-member-of-a-class/" rel="next" title="Python, member of a class">
                <i class="fa fa-chevron-left"></i> Python, member of a class
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/19/bit-manipulation/" rel="prev" title="A summary of Bit Manipulation">
                A summary of Bit Manipulation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/avatar.png"
               alt="虫二" />
          <p class="site-author-name" itemprop="name">虫二</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">81</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">33</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://github.com/iamtao" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/iamtaol" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      Weibo
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Tradeoff-batch-size-v-s-number-of-iterations"><span class="nav-number">1.</span> <span class="nav-text">Tradeoff batch size v.s. number of iterations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Ref-1"><span class="nav-number">1.1.</span> <span class="nav-text">Ref 1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ref-2"><span class="nav-number">1.2.</span> <span class="nav-text">Ref 2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ref-3"><span class="nav-number">1.3.</span> <span class="nav-text">Ref 3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ref-4-batch-size-related-to-hardware"><span class="nav-number">1.4.</span> <span class="nav-text">Ref 4: batch size related to hardware</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">虫二</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
